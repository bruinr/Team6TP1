---
title: "SVC and SVM"
author: "Team 6"
date: "3/10/2021"
output: 
  html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Support Vector Machines {.tabset}

## 9.6.1 Lab - SVC {.tabset}

The e1071 library contains implementations or a number of statistical 
learning methods. In particular , the **svm()** function can be used to  fit 
a  support vector classifier when the argument *kernel="linear"*
is used. 

This function uses a slightly different formulation from 9.14 and 9.25 for 
the support vector classifier 

A **cost argument** allows us to specify the *cost of violation* to the margin.
When the cost argument is *small*, then the margins will be *narrow* and there 
will be *few support* on the margin or violating the margin.

We now use the svm() function to fit the support vector classifier for a *given*
*value of the cost parameter*.


* Two dimensional example is presented below to plot the resulting 
decision boundary. 
+ 1. generate the  observations, which belong to two classes 
+ 2. check whether the classes **linearly separable**
```{r}
set.seed(1)
x=matrix(rnorm (20*2), ncol=2)
y=c(rep(-1,10), rep(1,10))
x[y==1,]=x[y==1,] + 1
plot(x, col=(3-y))

```
**They are not linearly separable**

Next we fit the **support vector classifier**
*Note that in order for the svm() function to perform classification *
*we must encode the response as a factor variable.*

We now create a data frame with the response coded as a factor.
```{r}
dat=data.frame(x=x, y=as.factor(y))
library(e1071)
svmfit=svm(y~., data=dat , kernel ="linear", cost=10, scale=FALSE)

```
The argument **scale=FALSE** 
* tells the **svm()** function *not to scale* each feature 
* to contain a __mean zero or standard deviation one__

Depending on the  application, one might prefer to use the __scale=TRUE__.

*Plot the support vector classifier:
```{r}
plot(svmfit , dat)
```
Note: The two arguments to the **plot.svm()** function are the
+ 1. output of the call to svm()
+ 2. data used in the call to svm(). 

* The region of feature space that will be assigned to the **-1 class**
is shown in *light blue*, 
* The  region that will be assigned to **+1 class** is shown in *purple*. 
* The decision boundary between the two classes is __linear__ 
(because we used the argument  *kernel="linear"* earlier), 

Plotting function makes the decision boundary look jagged in the first plot
due to the way it is implemented in the library.

*4=Note that here the second feature is plotted on the x-axis*
*and the first feature is plotted on the y-axis, in contrast to the*
*behavior of the usual plot() function in R.*

* **support vectors** are plotted as *crosses* 
* **remaining observations** are plotted as *circles*
We see here that there are seven support vectors. We can determine their 
identities as follows:


```{r}
svmfit$index
```
 Obtaining basic information about the support vector classifier fit using the **summary()** command:

```{r}
summary(svmfit)
```

Summary() function shows us:
* a linear kernel was used with the cost=10
* there are seven support vectors
+ four in one class 
+ three in the other

What if we instead used a *smaller* value of the cost parameter?

```{r}
svmfit=svm(y~., data=dat , kernel ="linear", cost =0.1, scale=FALSE)
plot(svmfit , dat)
svmfit$index
```

Utilization of Smaller Value of the Cost Parameter
* obtain a larger number of support vectors, because the margin is now wider. 


Unfortunately, the svm() function:
+ does not explicitly output the coefficients of the linear decision boundary 
obtained when the support vector classifier is fit
+ nor does it output the width of the margin.


The e1071 library includes a built-in function, **tune()**, to perform 
**cross-tune() validation**.
*By default **tune()** performs **ten-fold cross-validation **
on a set of models of interest
*In order to use this function:
+ we pass inrelevant information about the set of models that are under 
consideration
+ The following command indicates that we want to compare SVMs with a linear
kernel, using a range of values of the cost parameter.
```{r}
set.seed(1)
tune.out=tune(svm, y~.,data=dat, kernel ="linear", ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
```
_Cross-validation errors_
*Cross-validation errors for each model using summary() command:

```{r}
summary(tune.out)
```

_Analyzing summary function_

+ results for lowest cross-validation error rate: cost=0.1 

The **tune()** function stores the best model obtained, which can be accessed 
as follows:
```{r}
bestmod=tune.out$best.model
summary(bestmod)

```
_Predict function_

The **predict()** function can be used to predict the class label on a set of 
*test observations*, at any given value of the __cost parameter__. 

We begin by:
1. Generating a test data set.
```{r}
xtest=matrix(rnorm (20*2) , ncol=2)
ytest=sample (c(-1,1), 20, rep=TRUE)
xtest[ytest==1,]= xtest[ytest==1,] + 1
testdat=data.frame(x=xtest , y=as.factor(ytest))

```

Predict the **class labels** of these __test observations__. 


* Use the *best model obtained* through the **cross-validation** in order to 
make a prediction.


```{r}
ypred=predict (bestmod ,testdat)
table(predict =ypred , truth=testdat$y )
```

Thus, with this value of **cost**, 19 of the test observations are 
correctly classified. What if we had instead used **cost=.01**?

```{r}
svmfit=svm(y~., data=dat , kernel ="linear", cost =.01,
scale=FALSE)
ypred=predict (svmfit ,testdat )
table(predict =ypred , truth=testdat$y )

```
In this case one additional observation is misclassified. 

Consider the situation in which the *two classes are linearly separable*.
Then we can find a separating  hyperplane using the **svm()** function. 

1. Further separate the two classes in our simulated data so that they are 
linearly separable:

```{r}
x[y==1,]=x[y==1,]+0.5
plot(x, col=(y+5)/2, pch=19)

```
Fit and Plot
Now the observations are barely linearly separable.

2. We then _fit_ the **support vector classifier** 
3. _plot_ the resulting **hyperplane** using a large value for cost. 
-->This causes no observation to be misclassified.

```{r}
dat=data.frame(x=x,y=as.factor(y))
svmfit=svm(y~., data=dat , kernel ="linear", cost=1e5)
summary(svmfit)
plot(svmfit, dat)
```
No training errors were made, Only three support vectors were.

However,we can see from the figure that the **margin is narrow**. 
+ Due to the observations that are not support vectors, indicated as circles,
being _close to the decision boundary_.

This model seems likely to perform poorly on test data.
-> Try a **smaller**  value of cost:

```{r}
svmfit=svm(y~., data=dat , kernel ="linear", cost=1)
summary(svmfit)
plot(svmfit ,dat)
```
Using **cost=1**, we misclassify a training observation, 
but we also obtain a much **wider margin** and make use of support vectors. 
This model would be more likely to perform better on test data than the 
model at **cost=1e5**


## 9.6.2 Lab - Support Vector Machines {.tabset}

### **Non-Linear SVM Models** {.tabset}

SVM with **non-linear kernels**

In order to fit an SVM using a non-linear kernel, we once again use the **svm()** function. However, now we use a different value of the parameter kernel.

**Polynomial Kernel**: To fit an SVM with a polynomial kernel we use **kernel="polynomial"**

* Uses the **degree=** argument to specify a degree for polynomial

**Radial Kernel**: To fit an SVM model with a radial kernel we use **kernel="radial"**

+ Uses **gamma=** to specify a value of  for the radial basis kernel

Again, we want to remember that the *cost argument* allows us to specify the *cost of violation* to the margin

+ When cost is high, margins will be *narrow* and there will be *few support* on or violating the margin

**Goal of SVM**: Find a hyperplane, separating two classes of y values

<br>

This specific lab will look into a SVM model utilizing a radial kernel


We first generate some data with a *non-linear class boundary*, as follows:
```{r}
set.seed(1)
x = matrix(rnorm(200*2), ncol=2)
x[1:100, ] = x[1:100, ]+2
x[101:150, ] = x[101:150, ] -2
y = c(rep(1,150), rep(2,50))
dat = data.frame(x=x, y=as.factor(y))
```
  

Plotting the data makes it clear that the class boundary is indeed non-linear:

```{r }
plot(x, col=y)
```
**Not linearly separable**

<br>


From here, the data is randomly split into training and testing groups. We then fit the training data using the **svm()** function with a radial kernel and *gamma* = 1:

```{r}
train = sample(200,100)
svmfit = svm(y~., data=dat[train,], kernel="radial", gamma=1, cost = 1)
plot(svmfit, dat[train,])
```

The plot shows that the resulting SVM has a decidedly non-linear boundary. While it did a good overall job, we are still able to see at least 4 misclassifications

The summary() function can be used to obtain some information about the SVM fit:
```{r}
summary(svmfit)

```


As we observed earlier from the figure, there are a fair number of training errors present in this SVM fit. If we increase the value of cost, we can reduce the number of training errors. (*However, this comes at the price of a more irregular decision boundary that seems to be at risk of overfitting the data.*)
```{r}
svmfit=svm(y~., data=dat[train, ], kernel='radial', gamma=1, cost=1e5)

plot(svmfit, dat[train, ])

```

We can see in this example that the **increase of the cost parameter** from 1 to 100000, the shape goes from close to a circle to a less identifiable figure, that attempts to perfect a non-linear boundary

<br>

If we want to expedite the process of choosing the proper gamma and cost parameters for our model, we can perform cross-validation using **tune()**

+ **tune()** will select the best choice of *gamma* and *cost* for an SVM with a radial kernel 

+ We would use *degree* instead of *gamma* for a polynomial kernel


```{r}
set.seed(1)
tune.out = tune(svm, y~., data=dat[train, ], kernel="radial", ranges=list(cost=c(0.1,1,10,100,1000), gamma = c(0.5,1,2,3,4)))
summary(tune.out)
```
The book tells us the best choice involves a *cost* = 1 and *gamma* = 2, but our model shows us multiple locations where error is minimized (0.07) and provides us the first occurring in chronological order. 

This gives us a *cost* = 1 and *gamma* = 0.5

<br>


We can view the **test set predictions** for this model by applying the **predict()** function to the data. 
Note: To do this, we subset the dataframe, *dat*, using -train as an index set.
```{r}
table(true=dat[-train,"y"], pred=predict(tune.out$best.model, newdata=dat[-train,]))
```
The book gave us an example with a 50/50 split between the training and testing group, with the model resulting in 12% of observations being misclassified by this SVM with a radial kernel (Book misclassified 10%)





## Applied Exercise {.tabset}


### Objective {.tabset}

The goal of this exercise is to use a support vector classifier and support vector
machines (SVMs) to predict whether a car will get high or low gas mileage. To do so, 
we will use the Auto dataset from the ISLR library. The dataset contains 392 observations of 
9 variables. 

### Load Data {.tabset}
The Auto data comes from the ISLR library. We will also use various SVM functions 
from the e1071 library. 

```{r load, warning=FALSE}
rm(list=ls())
library(ISLR) #Auto data
library(e1071) #SVM functions
data(Auto)
```

### Prepare the Data {.tabset}
The goal of the problem is to predict a car's mileage (MPG) based on the other 
8 variables. In the original dataset, mpg is a continuous numeric variable. 
Recall that support vector classifiers and machines are typically applied to 
binary classification problems. Therefore, we must first create a new variable 
that is 0 if the MPG is below the dataset's median, and 1 otherwise. In order for 
the support vector classifier and machine to work, the variable must be coded as a factor.

```{r mpg}
#find median MPG
medianMPG <- median(Auto$mpg)
#code MPG as a factor
Auto$binaryMPG <- as.factor(Auto$mpg > medianMPG)
```

The new binaryMPG variable will be TRUE if the car has better gas mileage than 
the median, and FALSE otherwise. 

Now that the new representation of mpg has been created, we must remove the original
mpg from the dataframe. We also remove the name variable, since it is a factor that 
will introduce a large number of unwanted coefficients into the model. 


```{r clearmpg}
Auto <- subset(Auto, select=-c(mpg, name))
```

Finally, we want to scale the numeric columns to have a mean of 0 and a standard 
deviation of 1. This can be done automatically when creating one SVM model, but 
not when performing cross-validation, as we are about to do. 

```{r scale}
Auto$cylinders <- scale(Auto$cylinders)
Auto$displacement <- scale(Auto$displacement)
Auto$horsepower <- scale(Auto$horsepower)
Auto$weight <- scale(Auto$weight)
Auto$acceleration <- scale(Auto$acceleration)
Auto$year <- scale(Auto$year)
Auto$origin <- scale(Auto$origin)
```


### Support Vector Classifier {.tabset}
#### Explanation 
As a reminder, a support vector classifier (also known as a soft margin classifier)
creates a linear hyperplane to separate
the dataset into two regions. Now that the response variable (MPG) has been 
converted to a binary categorical form, we can apply a support vector classifier
to predict whether a car's MPG will fall below the median. 

In this case, we assume there is no hyperplane that will perfectly separate the data. 
Therefore, we must allow some observations to fall inside the margins of the hyperplane.
The number and severity of these margin violations is determined by the cost tuning
parameter. When the cost is small, each violation will incur a small penalty, so 
the margins will be large. When cost is large, each violation will be much more 
detrimental, so the margins will be very small. 
In the model, the optimal cost will be determined by cross-validation. 

To create the cross-validated model, we use the 'tune' function. Since SVCs are 
linear, we use a linear kernel. We also specify the type of model (in this case, SVM 
(support vector machine)), formula, dataset, and a list of possible costs. The general syntax is:

```
tune.out <- tune(svm, y~x, data=data_frame, kernel='linear', 
                 ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
```

Once the cross-validated models are created, we can select the model with the 
lowest error. (Since the cost does not affect the complexity of the model, the 
one-standard-deviation rule is not necessary). 

#### Model

``` {r svc}
set.seed(12)
tune.out <- tune(svm, binaryMPG~., data=Auto, kernel='linear', 
                 ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)), scale=TRUE)
summary(tune.out)

bestmod <- tune.out$best.model
summary(bestmod)
```

In this case, the best model has a cost of 0.01. The model misclassifies 9.17% of the 
observations (error), with a standard deviation of 5.09% (dispersion).
There are 166 support vectors, 
which means that 166 of the 392 observations lie directly on or on the wrong 
side of the hyperplane margin. A change in any of these 166 observations would 
affect the final result. 

#### Predictions

In this case, we did not split the data into training and testing sets. However, 
we can still determine the predictions and generate a confusion matrix to see 
where the model misclassifies entries. To obtain the predictions, we use the 
predict function with the correct model and the list of all X data. 

```{r lin_confusion}
yhat <- predict(bestmod, newdata = subset(Auto, select=-c(binaryMPG)))
table(predict=yhat, truth=Auto$binaryMPG)
```

Here, we can see that the model mostly correctly predicts whether a car will get 
good gas mileage. 

#### Plots (and why we shouldn't use them)

We can also use the plot() command to visualize the dataset and hyperplane. The 
syntax is: 
```
plot(model, dataframe, x1~x2, slice=list())
```
However, when there are more than two features, the plots may be difficult to interpret. 
In this case, it is necessary to use the slice argument to fix the values of the other 
features. Doing so means we see only a 2-dimensional slice of the 7-dimensional data.

```{r plotsvc}
plot(bestmod, Auto, weight~acceleration) 
```

This plot shows that most of the cars are predicted to have bad gas mileage (yellow),
but some cars with below-average weight may have good gas mileage. However, this assumes 
that all of the cars have the same displacement, horsepower, year, number of cylinders,
and origin (which is what we specified in the 'slice' argument). We can create a new plot
with different values of these parameters: 

```{r plotsvc2}
plot(bestmod, Auto, weight~acceleration, slice=list(displacement=-0.5, 
                                               horsepower=-0.5, 
                                               year=2,
                                               cylinders=0,
                                               origin=0))
```

Now, it looks like the model is predicting that most of the cars will have good 
gas mileage. Obviously, this is misleading- but it does serve as a good reminder 
of why we can't project a 7-dimensional space into 2 dimensions and expect to 
extract much useful information! 

### Polynomial Support Vector Machine {.tabset}

#### Model

The polynomial support vector machine (SVM) is the same as the linear SVC, but 
with a polynomial boundary between the regions instead of a linear one. The syntax
to make the model is the same as in the linear case; however, for this model, 
we will cross-validate to find the degree of the polynomial in addition to the cost.    

``` {r poly}
polysvm <- tune(svm, binaryMPG~., data=Auto, kernel='polynomial', 
                ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100), 
                            degree=c(2,3,4,5)))
summary(polysvm) 
```

A polynomial support vector machine applies the polynomial to support vectors, not 
features. Therefore, a higher-order polynomial does not result in a larger feature 
space or more complex model. This means we do not need to use the one-standard-deviation 
rule to determine the best model. Instead, we use the same code as above:

```{r bestpoly}
bestpoly <- polysvm$best.model
summary(bestpoly)
```

In this case, the best model is a degree-3 polynomial with a cost of 10. This model
misclassifies 7.15% of the observations (with a standard deviation of 3.38%). This
version has only 99 support vectors, instead of the 168 that were seen in the linear model. 

#### Plots

We can make another plot:

```{r plotpoly}
plot(bestpoly, Auto, weight~acceleration) 
```
Although the plotted boundary still does not accurately reflect the model predictions,
we can see that the shape is curved, instead of linear. 


### Radial Support Vector Machine {.tabset}

#### Model
Finally, a radial kernel may be used to draw a closed shape around one class of 
observations. Again, the syntax is the same as before, but this time we will also 
add cross-validation to find the optimal gamma. Gamma is a tuning parameter that determines
the effect that neighboring observations will have on classifying the observation in question.
If gamma is large, then distant observations will have almost no effect on the classification. 
If gamma is small, then more distant observations may affect the final prediction. 

``` {r radial}
radsvm <- tune(svm, binaryMPG~., data=Auto, kernel='radial', 
                ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100), 
                            gamma=c(0.5,1,2,3,4)))
summary(radsvm) 
bestrad <- radsvm$best.model
summary(bestrad)

```

Here, the best model has a cost of 1 and a gamma of 0.5, with 125 support vectors. 
This model misclassifies 6.88% of observations, with a standard deviation of 3.95%. 

#### Plots

Again, a plot: 

```{r plotrad}
plot(bestrad, Auto, weight~acceleration) 
```
Here, we can see the radial SVM boundary. 



## Poll Questions {.tabset}

We have created 5 poll questions to check our understanding of the different SVM models

### Questions (no correct) {.tabset}

1. What is the risk of increasing the cost attribute (as a means to reduce training error)?

  + a. A more simple decision boundary
  + b. The model becomes less accurate
  + c. Increased chance of overfitting data
  + d. Both A and C

2. Which argument is used to specify a tuning parameter for the *radial kernel* **only**?
  + a. Cost = 
  + b. Gamma = 
  + c. Degree = 
  + d. Kernel = 


3. True or False: a smaller value for the cost parameter will give the model a larger number of support vectors.
  + True
  + False
  
4. Within the context of support vector classifiers, which argument would we use to specify a penalty to a violation to the margin with a value of 10?
  + a. penalty = 10
  + b. cost = 10
  + c. weight = 10
  + d. error = 10

5. For a SVM, how do you determine the best values for the parameters (cost, gamma, & degree)?
  + a. Cross-Validation
  + b. Cross-validation and the one-standard deviation rule
  + c. They should be specified in the problem
  + d. Anova


### Q&A w/ explanation {.tabset}

1. What is the risk of increasing the cost attribute (as a means to reduce training error)?
  + **Correct Answer: C. Increased chance of overfitting the data**
  + Explanation: The decision boundary becomes more complex and irregular, eliminating option A and D, and we are unable to decide whether our model is more accurate yet until we run the test set. Therefore c is our best answer, because as we increase cost to reduce training error, we are potentially introducing bias and increasing the risk of overfitting the data.

2. Which argument is used to specify a tuning parameter for the *radial kernel* **only**?
  + **Correct Answer: B. Gamma = **
  + Explanation: Degree is used with the polynomial kernel; Cost = and Kernel = are not exclusive to the radial kernel. Gamma is used exclusively with the radial kernel


3. True or False: a smaller value for the cost parameter will give the model a larger number of support vectors.
  + **Correct Answer: True**
  + Explanation: A small cost means that there will be very wide margins, so many data points will be on the margin or within margin boundary, leading to many more support vectors
  
4. Within the context of support vector classifiers, which argument would we use to specify a penalty to a violation to the margin with a value of 10?
  + **Correct Answer: B. Cost = 10**
  + Explanation: We use the cost argument as the correct parameter to tune our model for support vector classifiers. 


5. For a SVM, how do you determine the best values for the parameters (cost, gamma, & degree)?
  + **Correct Answer: A. Cross-Validation**
  + Explanation: In the case of SVM we do not need to use the one standard deviation rule because it does not change the complexity of the model, they do not matter in SVM. Therefore, cross-validation is the correct answer. 

