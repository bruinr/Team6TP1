---
title: 'Lab'
author: "Team 6"
date: "3/6/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Support Vector Machines
The e1071 library contains implementations or a number of statistical 
learning methods. In particular , the **svm()** function can be used to  fit 
a  support vector classifier when the argument *kernel="linear"*
is used. 

This function uses a slightly different formulation from 9.14 and 9.25 for 
the support vector classifier 

A **cost argument** allows us to specify the *cost of violation* to the margin.
When the cost argument is *small*, then the margins will be *narrow* and there 
will be *few support* on the margin or violating the margin.

We now use the svm() function to fit the support vector classifier for a *given*
*value of the cost parameter*.


* Two dimensional example is presented below to plot the resulting 
decision boundary. 
+ 1. generate the  observations, which belong to two classes 
+ 2. check whether the classes **linearly separable**
```{r}
set.seed(1)
x=matrix(rnorm (20*2), ncol=2)
y=c(rep(-1,10), rep(1,10))
x[y==1,]=x[y==1,] + 1
plot(x, col=(3-y))

```
**They are not linearly separable**

Next we fit the **support vector classifier**
*Note that in order for the svm() function to perform classification *
*we must encode the response as a factor variable.*

We now create a data frame with the response coded as a factor.
```{r}
dat=data.frame(x=x, y=as.factor(y))
library(e1071)
svmfit=svm(y~., data=dat , kernel ="linear", cost=10, scale=FALSE)

```
### The argument **scale=FALSE** 
* tells the **svm()** function *not to scale* each feature 
* to contain a __mean zero or standard deviation one__

Depending on the  application, one might prefer to use the __scale=TRUE__.

*Plot the support vector classifier:
```{r}
plot(svmfit , dat)
```
#### Note: The two arguments to the **plot.svm()** function are the
+ 1. output of the call to svm()
+ 2. data used in the call to svm(). 

* The region of feature space that will be assigned to the **-1 class**
is shown in *light blue*, 
* The  region that will be assigned to **+1 class** is shown in *purple*. 
* The decision boundary between the two classes is __linear__ 
(because we used the argument  *kernel="linear"* earlier), 

Plotting function makes the decision boundary look jagged in the first plot
due to the way it is implemented in the library.

*4=Note that here the second feature is plotted on the x-axis*
*and the first feature is plotted on the y-axis, in contrast to the*
*behavior of the usual plot() function in R.*

* **support vectors** are plotted as *crosses* 
* **remaining observations** are plotted as *circles*
We see here that there are seven support vectors. We can determine their 
identities as follows:


```{r}
svmfit$index
```
### Obtaining basic information about the support vector classifier fit 
#### using the **summary()** command:

```{r}
summary(svmfit)
```
### Summary() function shows us:
* a linear kernel was used with the cost=10
* there are seven support vectors
+ four in one class 
+ three in the other

What if we instead used a *smaller* value of the cost parameter?

```{r}
svmfit=svm(y~., data=dat , kernel ="linear", cost =0.1, scale=FALSE)
plot(svmfit , dat)
svmfit$index
```
### Utilization of Smaller Value of the Cost Parameter
* obtain a larger number of support vectors, because the margin is now wider. 


Unfortunately, the svm() function:
+ does not explicitly output the coefficients of the linear decision boundary 
obtained when the support vector classifier is fit
+ nor does it output the width of the margin.


The e1071 library includes a built-in function, **tune()**, to perform 
**cross-tune() validation**.
*By default **tune()** performs **ten-fold cross-validation **
on a set of models of interest
*In order to use this function:
+ we pass inrelevant information about the set of models that are under 
consideration
+ The following command indicates that we want to compare SVMs with a linear
kernel, using a range of values of the cost parameter.
```{r}
set.seed(1)
tune.out=tune(svm, y~.,data=dat, kernel ="linear", ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
```
### Cross-validation errors for each model using summary() command:

```{r}
summary(tune.out)
```
### Analyzing summary function

+ results for lowest cross-validation error rate: cost=0.1 

The **tune()** function stores the best model obtained, which can be accessed 
as follows:
```{r}
bestmod=tune.out$best.model
summary(bestmod)

```
### Predict function:

The **predict()** function can be used to predict the class label on a set of 
*test observations*, at any given value of the __cost parameter__. 

We begin by:
1. Generating a test data set.
```{r}
xtest=matrix(rnorm (20*2) , ncol=2)
ytest=sample (c(-1,1), 20, rep=TRUE)
xtest[ytest==1,]= xtest[ytest==1,] + 1
testdat=data.frame(x=xtest , y=as.factor(ytest))

```
### Predict the **class labels** of these __test observations__. 


* Use the *best model obtained* through the **cross-validation** in order to 
make a prediction.


```{r}
ypred=predict (bestmod ,testdat)
table(predict =ypred , truth=testdat$y )
```

Thus, with this value of **cost**, 19 of the test observations are 
correctly classified. What if we had instead used **cost=.01**?

```{r}
svmfit=svm(y~., data=dat , kernel ="linear", cost =.01,
scale=FALSE)
ypred=predict (svmfit ,testdat )
table(predict =ypred , truth=testdat$y )

```
### In this case one additional observation is misclassified. 

Consider the situation in which the *two classes are linearly separable*.
Then we can find a separating  hyperplane using the **svm()** function. 

1. Further separate the two classes in our simulated data so that they are 
linearly separable:

```{r}
x[y==1,]=x[y==1,]+0.5
plot(x, col=(y+5)/2, pch=19)

```
###Now the observations are barely linearly separable.

2. We then _fit_ the **support vector classifier** 
3. _plot_ the resulting **hyperplane** using a large value for cost. 
-->This causes no observation to be misclassified.

```{r}
dat=data.frame(x=x,y=as.factor(y))
svmfit=svm(y~., data=dat , kernel ="linear", cost=1e5)
summary(svmfit)
plot(svmfit, dat)
```
### No training errors were made
Only three support vectors were.

However,we can see from the figure that the **margin is narrow**. 
+ Due to the observations that are not support vectors, indicated as circles,
being _close to the decision boundary_.

This model seems likely to perform poorly on test data.
-> Try a **smaller**  value of cost:

```{r}
svmfit=svm(y~., data=dat , kernel ="linear", cost=1)
summary(svmfit)
plot(svmfit ,dat)
```
###Using **cost=1**, 

we misclassify a training observation, 
but we also obtain a much **wider margin** and make use of support vectors. 
This model would be more likely to perform better on test data than the 
model at **cost=1e5**






