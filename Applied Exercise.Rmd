---
title: "Team 6 Applied Exercise"
author: "Carrington Metts"
date: "3/5/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Objective

The goal of this exercise is to use a support vector classifier and support vector
machines (SVMs) to predict whether a car will get high or low gas mileage. To do so, 
we will use the Auto dataset from the ISLR library. The dataset contains 392 observations of 
9 variables. 

## Load Data
The Auto data comes from the ISLR library. We will also use various SVM functions 
from the e1071 library. 

```{r load, warning=FALSE}
library(ISLR) #Auto data
library(e1071) #SVM functions
data(Auto)
```

## Prepare the Data
The goal of the problem is to predict a car's mileage (MPG) based on the other 
8 variables. In the original dataset, mpg is a continuous numeric variable. 
Recall that support vector classifiers and machines are typically applied to 
binary classification problems. Therefore, we must first create a new variable 
that is 0 if the MPG is below the dataset's median, and 1 otherwise. In order for 
the support vector classifier and machine to work, the variable must be coded as a factor.

```{r mpg}
medianMPG <- median(Auto$mpg)
Auto$binaryMPG <- as.factor(Auto$mpg > medianMPG)
str(Auto)
```

Now that the new representation of mpg has been created, we must remove the original
mpg from the dataframe. 

```{r clearmpg}
Auto <- subset(Auto, select=-c(mpg))
```

## Support Vector Classifier 
As a reminder, a support vector classifier (also known as a soft margin classifier)
creates a linear hyperplane to separate
the dataset into two regions. Now that the response variable (MPG) has been 
converted to a binary categorical form, we can apply a support vector classifier
to predict whether a car's MPG will fall below the median. 

In this case, we assume there is no hyperplane that will perfectly separate the data. 
Therefore, we must allow some observations to fall inside the margins of the hyperplane.
The number and severity of these margin violations is determined by the cost tuning
parameter. When the cost is small, each violation will incur a small penalty, so 
the margins will be large. When cost is large, each violation will be much more 
detrimental, so the margins will be very small. 
In the model, the optimal cost will be determined by cross-validation. 

To create the cross-validated model, we use the 'tune' function. Since SVCs are 
linear, we use a linear kernel. We also specify the type of model (in this case, SVM 
(support vector machine)), formula, dataset, and a list of possible costs. The general syntax is:

```
tune.out <- tune(svm, y~x, data=data_frame, kernel='linear', 
                 ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
```

Once the cross-validated models are created, we can select the model with the 
lowest error. (Since the cost does not affect the complexity of the model, the 
one-standard-deviation rule is not necessary). 

``` {r svc}
set.seed(12)
tune.out <- tune(svm, binaryMPG~., data=Auto, kernel='linear', 
                 ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out)

bestmod <- tune.out$best.model
summary(bestmod)
```

In this case, the best model has a cost of 1. The model misclassifies 9.29% of the 
observations (error), with a standard deviation of 3.61% (dispersion).
There are 300 support vectors, 
which means that 300 of the 392 observations lie directly on or on the wrong 
side of the hyperplane margin. A change in any of these 300 observations would 
affect the final result. 

```{r plotsvc}
plot(bestmod, Auto, weight~year)
```

## Polynomial Support Vector Machine

The polynomial support vector machine (SVM) is the same as the linear SVC, but 
with a polynomial boundary between the regions instead of a linear one. The syntax
to make the model is the same as in the linear case; however, for this model, 
we will cross-validate to find the degree of the polynomial in addition to the cost.    

``` {r poly}
polysvm <- tune(svm, binaryMPG~., data=Auto, kernel='polynomial', 
                ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100), 
                            degree=c(2,3,4,5)))
summary(polysvm) 
```

## Radial Support Vector Machine
Finally, a radial kernel may be used to draw a closed shape around one class of 
observations. Again, the syntax is the same as before, but this time we will also 
add cross-validation to find the optimal gamma. Gamma is a tuning parameter that determines
the effect that neighboring observations will have on classifying the observation in question.
If gamma is large, then distant observations will have amost no effect on the classification. 
If gamma is small, then more distant observations may affect the final prediction. 

``` {r radial}
radsvm <- tune(svm, binaryMPG~., data=Auto, kernel='polynomial', 
                ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100), 
                            gamma=c(0.5,1,2,3,4)))
summary(radsvm) 
bestrad <- radsvm$best.model
summary(bestrad)
```


